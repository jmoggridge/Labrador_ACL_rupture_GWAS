---
title: "Genome-wide association tutorial: finding SNPs associated with ACL rupture in Labrador retrievers using `plink`"
author: "Jason Moggridge"
date: "`r Sys.Date()`"
output:
  pdf_document:
    highlight: kate
    toc: true
    toc_depth: 3
urlcolor: purple
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message =F, warning = F, cache = F)
```

---

# Introduction

This tutorial shows how to perform a case-control genome-wide association analysis (GWAS) of SNP data for ACL rupture in Labrador retrievers, using the popular [`plink` toolset](https://zzz.bwh.harvard.edu/plink/index.shtml). The sample dataset, from a recent work by [Baker *et al.* (2017)](https://doi.org/10.1371/journal.pone.0173810), consists of 173,662 SNPs and 237 dogs and this is available through data dryad. 
The process involves performing quality control for a number of characteristics, exploratory analysis of data with MDS, and finally testing for genome-wide association. This tutorial also demonstrates how to visualize the results with `R` using `ggplot2` . 

**Related publication**: 
Baker et al. 2017. Genome-wide association analysis in dogs implicates 99 loci as risk variants for anterior cruciate ligament rupture. *PLOS ONE* 12(4): e0173810. 
https://doi.org/10.1371/journal.pone.0173810
  
  **Data source**: https://datadryad.org/stash/dataset/doi:10.5061/dryad.8kk06   

#### GWAS analysis outline

 - 1 - QC + filtering data
 - 2 - Population stratification & MDS 
 - 3 - Association tests and statistics

#### Plink usage primer

    plink --bfile {my_data} --dog --options --commands --out {new_name} 

Plink commands are run in the unix shell. Throughout this tutorial, we use the `--bfile` flag because we have bed/bim/fam files; the `--make-bed` flag to get the same type of files back; and we always have to use the `--dog` flag because dogs have 38 autosomes (so many!) and plink expects human data otherwise (for every command else you'll get an error!). For various filtering tasks, plink has `--keep` & `--remove` commands for filtering individuals, while the `--extract` & `--exclude` functions do the same for variants. Each of these requires a input file with a list of identifiers, eg. <`--keep keep_list.txt`> . This tutorial can only provide a short explanation of each function and the parameters used, but you will certainly want more information: check out the [plink manual](https://zzz.bwh.harvard.edu/plink/reference.shtml) to get more insight into each function as you work your way through the tutorial.


# Step-by-step tutorial

All commands should be run in the unix shell, except for in the final section, 'Visualize the results', where we will do plotting in R. Commands that you should run are indicated with syntax highlighting. 

## 1 - Data and software

Let's get the dataset from data-dryad using `curl`. Do this on a login node on `graham`. The files are relatively small. (If the `curl` throws an error, try: exiting graham, ssh in again, run curl again)

```{r download, engine = 'bash', eval = F, include = T}
curl -L http://datadryad.org/api/v2/datasets/doi%253A10.5061%252Fdryad.8kk06/download \
  --output labrador_download
unzip labrador_download
```

We got the cr237_dryad.bed, .bim., and .fam files: 'cr237_dryad.bed': has genotype data; 'cr237_dryad.bim': has info about each variant (human-readable); 'cr237_dryad.fam': has phenotype data (human-readable). See the plink documentation for [more info about file formats](https://www.cog-genomics.org/plink/1.9/formats) (other input formats are possible, if you're working with files that don't match this pattern).


```{r famfile, engine = 'bash', eval = F, include = T}
# check out the individuals data with phenotypes
head cr237_dryad.fam
```

There are no relatives in this dataset, so the FIDs are all the same as the IIDs (cols 1+2), and father & mother IDs (cols 3 and 4) are all zeros. Our phenotype (ACL rupture) is in column 6.   
\


----

## 2 - QC / Filtering

**Start an srun session now**, load these modules to get plink and R setup.

```{r modules, engine = 'bash', eval = F, include = T}
module load nixpkgs/16.09 gcc/7.3.0 r/4.0.2 plink/1.9b_4.1-x86_64 
```

#### a. Data missingness:

First we'll check the amount of missing data per individual with `--missing`.

```{r missing, engine = 'bash', eval = F, include = T}
plink --dog --bfile cr237_dryad --missing
```

The files with suffixes .imiss and .lmiss show the proportion of missing data per loci and individual. There is code for plotting these data in the appendix. Now we'll filter any SNPs that are missing > 5% with `--geno`, and then we'll do the same for individuals with `--mind`. 

```{r filter_miss, engine = 'bash', eval = F, include = T}
# filter SNPs
plink --dog --bfile cr237_dryad --geno 0.05 --make-bed --out cr237_filter1
# filter individuals 
plink --dog --bfile cr237_filter1 --mind 0.05 --make-bed --out cr237_filter1
```

You can see how many were dropped in both steps (7468 variants, 0 dogs) from plink's output to the terminal (as well as other useful data for tracking our dataset)  You'll see that we get warnings about '25567 het. haploid genotypes', 'Nonmissing nonmale Y' - we'll deal with that next.


#### b. Sex discrepancy check:

We can use the genotype data to check whether the recorded sex for each individual makes sense. This is based on looking at SNPs on the X chromosome, but the X has a large pseudo-autosomal region that we need to ignore first (6630000 - 126883977). In the dog dataset we see some unusual results, so we'll use plink to impute the sex of 6 dogs.

To ignore SNPs in the canine pseudo-autosomal region (PAR) of X-chromosome we use --splitx and provide the boundaries of the PAR

```{r splitX, engine = 'bash', eval = F, include = T}
# ignore X-chromosome PAR 
plink --dog --bfile cr237_filter1 --split-x 6630000 126883977 \
  --make-bed --out cr237_splitx
```

568 chromosome codes changed here, which is very close to the 565 SNPs in the PAR reported [in this paper I found](https://www.genetics.org/content/184/2/595#T3). Note: the position range given above could be inaccurate, as I took the numbers from a [google groups post](https://groups.google.com/g/plink2-users/c/jvLBBkjbX84?pli=1). The number of heterozygous haploid genotypes decreased to 5284 as a result of updating the way plink handles the PAR variants.

We will now use F-values produced by `--check-sex` (metric of X chromosome homozygosity/ inbreeding) to inform us if any individuals are sexed incorrectly or ambiguously. We add arguments for the threshold for females and males to be flagged. These are set very loosely here; the default thresholds will OK females if $F < 0.2$ and males if $F > 0.8$. In a histogram of the F-value distributions, males should appear near 1, females more loosely distributed around 0. 

```{r sex_check, engine = 'bash', eval = F, include = T}
# check sex labels
plink --bfile cr237_splitx --dog --check-sex 0.799 0.80 
```

The output tells us that 6 problems are detected with these (very relaxed) thresholds. We plot the sex-check results in the R section below, where we see that there are some outliers in both sexes.

One option available to us is to impute the sex of ambiguous individuals with `--impute-sex`. As before, thresholds are given for labeling any problem cases. We do `--check-sex` again after imputation to get the new results with imputed labels.

```{r impute_sex, engine = 'bash', eval = F, include = T}
plink --dog --bfile cr237_splitx --impute-sex 0.799 0.80   \
  --make-bed --out cr237_splitx_impute
  
plink --dog --bfile cr237_splitx_impute --check-sex 0.799 0.80 \
  --out imputed
  
grep "PROBLEM" imputed.sexcheck # no more problems
```


We'll go with imputation of the 6 problematic labels, keeping all subjects. Perhaps the F-values for dogs can be bit more variable than the expectations outlined above (which are probably suited specifically to human genotypes).


#### c. Autosomal SNPs only:

Generally, we only want to analyze the autosomal SNPs, not the X, Y, or mitochondrial ones.  Here we use `awk` to output a list of SNPs that are on chromosomes 1-38 (dog autosomes) from the .bim file. The we use `--extract` to retrain only these SNPs in the output `cr237_filter2`.

```{r autosomes, engine = 'bash', eval = F, include = T}
awk '{ if ($1 >= 1 && $1 <= 38) print $2 }' cr237_splitx_impute.bim \
  > snp_1_38.txt

plink --dog --bfile cr237_splitx_impute --extract snp_1_38.txt \
  --make-bed --out cr237_filter2
```

Great, we removed ~ 6k SNPs and now only have autosomal SNPs for our GWAS.

#### d. MAF filtering:  

Generally, we want a threshold of MAF > 0.05, or possibly lower but only if our sample is very large. We can use `--freq` to get the MAF statistics for all SNPs, and `--maf` to filter the dataset to a specified threshold (5%). 

```{r maf, engine = 'bash', eval = F, include = T}
# Get the minor allele frequencies for each SNP
plink --bfile cr237_filter1 --dog --freq --out MAF_check
# Remove SNPs with MAF < 5%
plink --bfile cr237_filter2 --dog --maf 0.05 --make-bed --out cr237_filter3
```

We removed 45k SNPs with small minor allele frequency. The results from --freq (MAF_check.frq) can be plotted to show the minor allele frequency distribution (see the R section).


#### e. Hardy-Weinberg equilibrium:

Do we have far more heterozygous calls than expected? That could mean there is systemic calling error (bad). If we have fewer heterozygotes than expected, it could be due to population stratification (also bad if not controlled). GWAS methods generally assume HW-equilibrium for allele frequencies, so it is important to remove any SNPs that are far from equilibrium.

First we use `--hardy` to output the HWE exact test p-values to the file plink.hwe for plotting (see R section). Afterwards, we use `--hwe` to remove SNPs that deviate from HWE with p-value smaller than 1e-7 (as Baker *et al.* did). By default, `--hwe` only removes these based on the control group, so we repeat the call with the `--hwe-all` flag to remove them for the cases too. 

```{r HWE, engine = 'bash', eval = F, include = T}
# get HWE exact test values
plink --dog --bfile cr237_filter3 --hardy 
# filter by HWE in controls
plink --dog --bfile cr237_filter3 --hwe 1e-7 --make-bed --out cr237_filter4
# filter by HWE in cases
plink --dog --bfile cr237_filter4 --hwe 1e-7 --hwe-all \
  --make-bed --out cr237_filter4
```

The two filtering steps removed 43 and 93 variants.

#### f. SNP pruning \

We will now 'prune' the set of SNPs to get a subset consisting of loci that are in approximate linkage equilibrium. This removes a bunch of SNPs that are highly correlated and thus provide little extra information for association testing. We do this with `--indep-pairwise`, where three parameters `50 5 0.2` are given to scan a window size 50 kbp, to shift the window 5 SNPs at each step, and to use 0.2 as the pairwise $r^2$ threshold. 

```{r prune_snps, engine = 'bash', eval = F, include = T}
plink --bfile cr237_filter4 --dog --indep-pairwise 50kb 5 0.2 --out indepSNP
```

The output file indepSNP.prune.in has the subset of independent SNPs to retain and .out has 41k excluded SNPs. We'll use the indepSNP.prune.in file to only consider the pruned set of SNPs in future steps (don't delete this).

#### g. Heterozygosity \

Now with the pruned set of SNPs we want to remove any individuals with heterozygosity more than 3 sd from the mean. We use `--het` to check compute this statistic.

```{r het_check, engine = 'bash', eval = F, include = T}
plink --dog --bfile cr237_filter4 --extract indepSNP.prune.in --het \
  --out prune_check
```

We need to do some computation here to find the individuals that are > 3 sd away from the mean, which is more easily done in R. Open nano and make an Rscript called 'identify_het_outliers.R' with the following code.

```{r write_het_script, engine = 'bash', eval = F, include = T}
nano identify_het_outliers.R
```

Put the following R code into the script. It filters the results to find any individuals too far from the mean.

```{r het_script_code}
df <- read.table("./prune_check.het", header = T)
df$het <-  (df$"N.NM." - df$"O.HOM.") / df$"N.NM."
mean_het <- mean(df$het)
sd_het <- sd(df$het)

fails <- subset(
  df, (df$het < mean_het - 3*sd_het) | (df$het < mean_het + 3*sd_het)
)
fails$dst <- (fails$het - mean_het) / sd_het
write.table(fails, "hetero_outliers.txt", row.names = FALSE)

```

Save the script and exit nano. Set permissions with `chmod` and run the Rscript with `--no-save` to avoid creating any work-space files.

```{r R_het_filter, engine = 'bash', eval = F, include = T}
chmod u+x identify_het_outliers.R 
Rscript --no-save identify_het_outliers.R 
```

This will generate the file hetero_outliers.txt.
We have to reformat the file with `sed` to remove the quotation marks that R adds to the header and `awk` to take only the FID and IID columns. The list then gets passed to plink `--remove` to take those individuals out.

```{r remove_het_outliers, engine = 'bash', eval = F, include = T}
sed 's/"// g' hetero_outliers.txt | awk '{print$1, $2}' > hetero_outliers.txt

plink --dog --bfile cr237_filter4 --remove hetero_outliers.txt \
  --make-bed --out cr237_filter5
```

Note that the list made by the Rscript is empty - we didn't have any individuals that needed to be removed but if we did, they would now be excluded.


#### h. Cryptic relatedness / Identity by descent:

Since an independent sample is assumed for association testing, we want to exclude any related individuals from analysis. Here we pi_hat statistic for identity by descent and examine data for individuals with high relatedness. Normally, we remove any individuals having 'cryptic relatedness', where pi_hat is greater than some threshold. In their article, Baker *et al.* don't mention doing this at all, so we may get different results by applying this filter.

The `--genome` function is used to compute identity by descent between pairs of individuals. The `--min` flag sets minimum PI_HAT for the output (otherwise we get all values). A PI_HAT of 1 implies identical individuals (duplicates in data), 0.5 is parent-child or siblings, 0.25 is aunt/uncle-niece/nephew. We should scrutinize anything over 0.2 as these pairs could be related. 

```{r genome_pihat, engine = 'bash', eval = F, include = T}
plink --dog --bfile cr237_filter5 --extract indepSNP.prune.in \
  --genome --min 0.2 --out pihat_min0.2
  
wc -l pihat_min0.2.genome
```

82 pairs of dogs have pi_hat > 0.2. If we were being rigorous, we'd probably want to remove one individual from each pair (see https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6001694/ for a more in-depth explanation). Otherwise, we might account for known relationships in the association model (as per Baker *et al.*) but that is beyond our scope. Here we remove one individual from a pair that are likely parent-child or siblings. 

We can filter the .genome results on Z1 values to find any pairs that likely represent siblings (Z1 > 0.9) as follows:

```{r siblings, engine = 'bash', eval = F, include = T}
awk '{ if ($8 >0.9) print $0 }' pihat_min0.2.genome > siblings.genome
cat siblings.genome
```

This pair of dogs could be siblings or parent-child based on their Z1 value. We should remove one of these individuals from our analysis, preferably the one with less missing data. 

We'll reshape a list of ids from the siblings.genome file and get the missing data for these individuals

```{r sib_missing, engine = 'bash', eval = F, include = T}
grep -v "FID" siblings.genome > siblings.genome2
awk '{print$1, $2}' siblings.genome2 | uniq > siblings.txt
awk '{print$3, $4}' siblings.genome2 | uniq >> siblings.txt

plink  --dog --bfile cr237_filter5 --keep siblings.txt --missing --out siblings 

head siblings.imiss
```

We could write code to select the individual with less missing data programmatically, but since there is only one pair, we can easily tell that individual 1251 has 1% more missing data than 1250.

For the next step, write "1251 1251" in a file called 'drop_this.txt', then pass it to `--remove` to drop this individual from our data.

```{r filter6, engine = 'bash', eval = F, include = T}
plink --dog --bfile cr237_filter5 \
  --remove drop_this.txt \
  --make-bed --out cr237_filter6
```

**We are finally done filtering! That took forever, right!?**  

Don't worry, we are getting to the good stuff now. This is a good time to download the plink.* files to your local machine for plotting (eg. with `scp`). We generated a lot of intermediate files, so I suggest removing these to keep the work-space tidy. You can safely remove these as follows:

```{r delete_stuff, engine = 'bash', eval = F, include = T}
for i in 1 2 3 4 5 ; do rm *_filter$i.*; done
rm sex_discrepancy.txt snp_1_38.txt \
  *splitx* het_fail_ind.txt hetero_outliers.txt \
  missing2* siblings* drop_this.txt
```


---

## 3 - Multidimensional scaling

If we have resources to place our study subjects within their  'ethnic' groups (eg. 1000 genomes SNPs data for human ethnicity), then we can 'anchor' our samples with this data to find out where the samples lie in relation to these groups. MDS is used to display the distance between individuals, such that we can identify any individuals that are outliers from their groups. Because we want to do our GWAS on a homogeneous population, we would remove any samples that deviate from the group based on a selected threshold. For more details about anchoring your data, [click here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6001694/) to see an example of MDS with human SNP data anchored by the 
1000 genomes data. There is more information about other [plink clustering options here](https://www.cog-genomics.org/plink/1.9/strat). 

Since we don't have data for other breeds handy, we'll perform MDS to check for any obvious outliers without anchoring. We also use the MDS components as covariates to control for population stratification in some association testing later. We'll plot the MDS for visual inspection (see appendix).

First, calculate the identity-by-sequence distances for all pairs of individuals.

```{r IBS, engine = 'bash', eval = F, include = T}
plink --dog --bfile cr237_filter6 \
  --extract indepSNP.prune.in \
  --genome --out cr237_filter6
```

Now do MDS clustering of individuals based on their autosomal SNP genotypes. We pass the IBS data along with `--read-genome`. Plink's
`--cluster` does complete linkage hierarchical clustering by default,
so we pass the `--mds-plot` flag to get MDS instead and tell it to give us the first 10 coefficients.

```{r MDS, engine = 'bash', eval = F, include = T}
plink --dog --bfile cr237_filter6 \
  --read-genome cr237_filter6.genome \
  --cluster --mds-plot 10 \
  --out cr237_filter6
```

The MDS results are in `cr237_filter6.mds`. You'll want to visualize these before moving on, so download the .mds and .fam files and skip to the appendix for MDS plotting. Check the results and return back to here afterwards. In the first two MDS components we can vaguely see two groups, but no individuals are extremely far from the cloud. 
We don't *need* to, but we *could* exclude the couple dogs that
are within -0.7 to -1 in component 2, for example.

Finally, we want to reformat the .mds file to a plink covariate file that we'll use to control for population stratification in the association analysis. 

```{r MDS_covars, engine = 'bash', eval = F, include = T}
awk '{print$1, $2, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13}' \
  cr237_filter6.mds > covar_mds.txt
```


---

## 4 - Association analysis


Association testing for a binary trait (ACL rupture in our case) compares the allele frequencies between the case and control groups. There are many tests for many parameters, so you should definitely visit the [plink association analysis](https://www.cog-genomics.org/plink/1.9/assoc) page to learn more.

### Basic association test  

We first do simple a Chi-squared test with one degree of freedom using the `--assoc` function, with `--ci 0.95` added so that we also get the 95% CI on the odds ratio (OR) for each SNP.  It's important to note that this testing method does not include correction for the MDS covariates (which is undesirable).

```{r assoc, engine = 'bash', eval = F, include = T}
plink --dog --bfile cr237_filter6 \
  --extract indepSNP.prune.in \
  --assoc --ci 0.95\
  --out result1
  
head result1.assoc
```

We can see that this gets us the Chi-sq, p-value, and odds ratio statistics for each SNP.  

We can also get various corrections for multiple testing: Bonferroni correction and FDR-based statistics can be obtained by calling `--assoc` with the `--adjust` flag. We get an extra file '*.assoc.adjusted' with the output.

```{r assoc_adj, engine = 'bash', eval = F, include = T}
plink --bfile cr237_filter6 --dog \
  -assoc --adjust --ci 0.95 \
  --out result_adjusted

# see the two files, one has the Chi^2 tests, the other has corrected
# p-values by various methods and is sorted by significance.
head result_adjusted.assoc
head result_adjusted.assoc.adjusted

```

Note: there aren't any (!) significant results after some corrections. Baker *et al*. note that Bonferroni correction is too stringent given that dogs have many SNPs that are inherited in haplotype blocks due to extensive linkage disequilibrium. 


### Logistic regression modeling   

We *do* want to take the MDS covariates into account and the --assoc function doesn't do this, so we can use Plink's `--logistic` function to do logistic regression with the MDS components included as covariates (passed with `--covar`). We also provide the `--hide-covar` flag to only get the results for the SNPs (not the MDS covariates).

```{r logistic, engine = 'bash', eval = F, include = T}
plink --dog --bfile cr237_filter6 \
  --extract indepSNP.prune.in \
  --covar covar_mds.txt \
  --logistic --ci 0.95 \
  --hide-covar \
  --out result2

head result2.assoc.logistic 
# Check if there are any NA values in the results
awk '/'NA'/' result2.assoc.logistic | wc -l 
# Remove any NA values from results
awk '!/'NA'/' result2.assoc.logistic  > result2.assoc.logistic
```

At this stage, you'll want to make a Manhattan plot of the association p-values - but get the permutation running first! Then you'll have some time (~30mins) on your hands to go to the R section and do the Manhattan plot.

```{r, echo = F, fig.width=8, fig.height=3}
library(tidyverse)
read_rds("./fig_manhat_for_report.rds") +
  labs("Genome-wide association significance for ACL rupture with logistic regression test")
```

### Permutation testing

This involves randomly permuting the phenotypes to get estimates of  significance empirically and as such it is computationally intensive. This is a general procedure and various tests can be applied in this framework; [go here learn more](https://zzz.bwh.harvard.edu/plink/perm.shtml) about the different uses of plink --perm. For more background, see this [article by Che *et al.*](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4070098/).

In the context of our dataset (small size, possibly contains relatives since we did not filter based on pi_hat), permutation is an attractive option to avoid some of the assumptions of ANOVA testing. Here we do 1 million permutations of the genotypes with `--assoc` and the `--mperm` option plus our number of repeats. This will invoke the max(T) algorithm rather than the adaptive one. You may set the `--seed` to any large integer so that you get the same results (there is randomness otherwise).


```{r perm, engine = 'bash', eval = F, include = T}
# Perform 1M perrmutations.
Plink --bfile cr237_filter6 --dog --seed 6377474 \
  --assoc --mperm 1000000 --ci 0.95 --out result_perm_1M
  
# Order by p-value
sort -gk 4 result_perm_1M.assoc.mperm > sorted_subset.txt
# Check results out; EMP2 has the corrected values
head sorted_subset.txt
```

Permutation will take a few minutes. None of the adjusted p-values are significant (spoiler).


# Discussion 

There are a number of complicated aspects in performing genome-wide association analyses. Many genetic and statistical concepts are involved and organism-specific knowledge is key. I found that setting parameters for pruning and selecting appropriate association tests was challenging. The dataset I chose for this tutorial (Baker *et al.,* 2017) had the benefit of being a nice size (n=237), though I could not reproduce their result as some data is missing from data dryad (*e.g.*. neutering). I found Plink easy to use after following the [Marees GWAS tutorial](https://github.com/MareesAT/GWA_tutorial/). The documentation is adequate to operate the tool but the explanation is terse, as familiarity with GWAS and statistical tests is assumed. The tutorial I followed helped somewhat but used simulated human data. As such I had to adapt their work-flow for canine data.

I choose to perform a GWAS analysis for this work because I am interested in methods that allow us to unravel the genetic underpinnings of complex traits and diseases. Genome-wide association and quantitative trait loci both seem highly relevant as large-scale genomics becomes more tractable, allowing personalized medicine, *etc.*, to be realized.


----


# Appendix - Visualizing the results 

This section covers how to parse and plot the Plink results with R in the `tidyverse` framework. The data from Plink are fairly easy to work with but have an unusual space-delimited format and the column names contain white-spaces when they are read in. The `gwa_*` functions here can be plot any data of the right type (*ie.* .suffix), though you may need to edit some layers if you have much more data than the Labradors example. Since the figures are ggplot2 objects, you can easily customize plot appearance to your liking with the usual ggplot2 stuff: theme_\*(), theme(), labs(), *etc*. 

I am assuming the reader has some experience with `dplyr` and `ggplot2`. Any other packages used are indicated with the `package::function` notation. Should you prefer base R plotting, then see the [Marees GWAS tutorial](https://github.com/MareesAT/GWA_tutorial/) which has code to do that. 


Install the following packages if you don't already have them (with `install.packages()`)

```{r libraries, include=T, eval=T}
library(tidyverse)  
library(patchwork)    # to glue plots together
library(ggrepel)      # for labels on manhattan plot
```


#### QC Missing data:

These functions plot the distribution of the proportion of missing data for individuals and SNPs from the plink.imiss and plink.lmiss files (from QC step 'a').

```{r missing_hist, fig.width=6, fig.height=2.5}
# plots histogram of missing data for individuals from .imiss file
gwa_missing_indiv <- 
  function(plink_imiss){
    plink_imiss %>% 
      janitor::clean_names() %>% 
      transmute(f_miss = as.numeric(f_miss)) %>% 
      ggplot(aes(f_miss)) +
      geom_histogram() +
      labs(subtitle = "Missing data per individual", 
           x = 'proportion missing',
           y = 'individuals')
  }

# plots histogram of missing data for SNPs from .lmiss file
gwa_missing_snp <- 
  function(plink_lmiss){
    plink_lmiss %>% 
      janitor::clean_names() %>% 
      transmute(f_miss = as.numeric(f_miss)) %>% 
      ggplot(aes(f_miss)) +
      geom_histogram() +
      labs(subtitle = "Missing data per SNP", 
           x = 'proportion missing',
           y = 'SNPs')
  }

# read data and make plots
indiv <- read_delim("./plink.imiss", delim = ' ') %>% 
  gwa_missing_indiv() +
  theme_bw()
snps <- read_delim("./plink.lmiss", delim = ' ') %>% 
  gwa_missing_snp() +
  theme_bw()
# combine plots with patchwork + operator
indiv + snps
```


---


#### QC Sex-check: 

This will plot the F-values from the plink.sexcheck results created in QC step 'b'.


```{r sex_check_hist, fig.height=2.5, fig.width=4}

# plots plink --sex-check results
gwa_sexcheck <- function(plink_sexcheck, plt_title){
  plink_sexcheck %>% 
  janitor::clean_names() %>% 
  transmute(f_value = as.numeric(f),
            pedsex = as.numeric(pedsex),
            pedsex = case_when(
              pedsex == '1' ~ "male",
              pedsex == '2'~ "female",
              TRUE ~ "labeled as ambiguous"),
            ) %>% 
  ggplot(aes(x = f_value)) +
  geom_histogram() +
  facet_wrap(~pedsex, ncol = 2) +
  theme(legend.position = 'NA')
}

# read and plot .sexcheck data; add more layers to ggplot objects
# to customize, using + labs(), + theme(), etc.
read_delim("./plink.sexcheck", delim = ' ') %>% 
  gwa_sexcheck() +
  theme_bw() +
  labs(
    x = 'X-chromsome inbreeding coefficient',
    y = 'dogs',
    subtitle =  "--sex-check: input data vs SNP-imputed sexes. 
    Outliers could be labelled incorrectly in the input data"
    ) 

```




#### QC Minor allele frequency:

This plots the distribution of MAF from MAF_check.frq results obtained in QC step 'd'.

```{r maf_hist}

maf <- read_delim("./MAF_check.frq", delim = ' ') %>% 
  janitor::clean_names() %>% 
  ggplot(aes(x = as.numeric(maf))) +
  geom_histogram() +
  theme_bw() +
  labs(x = 'MAF', y = 'SNPs', 
       subtitle = "Minor allele frequency distribution")
```

#### QC: Hardy-Weinberg equilibrium exact test:

This plots the distribution of HWE exact test p-values from QC step 'e'.

```{r HWE_plot}
hwe <- read_delim("./plink.hwe", delim = ' ') %>% 
  janitor::clean_names() %>% 
  transmute(p = as.numeric(p)) %>% 
  ggplot(aes(x = p, fill = if_else(p < 1e-07, '< 1e-7', 'keep'))) +
  geom_histogram() +
  theme_bw() +
  labs(x = 'p-value', y = 'SNPs', fill = '',
       subtitle = "Hardy-Weinberg equilibrium test")
```

```{r maf_hwe, fig.height=2.5, fig.width=7.5}
maf + hwe 
```


----

#### MDS Plot:

These functions create a 2d plot of the MDS (`gwas_mds1()`), and a multipanel plot of the first 5 components (`gwas_mds2`). These require joined the .mds and .fam data as you will see below.

```{r mds_plots}
# Plot the first 2 components of the MDS analysis
gwas_mds1 <- function(mds_data){
  dogs %>% 
    ggplot(aes(c1, c2, color = phenotype, shape = sex)) +
    geom_point() +
    coord_equal() +
    theme_minimal() +
    theme(panel.grid = element_blank(), 
          panel.border = element_rect(color = 'black', 
                                      size = 0.5, fill = NA)) +
    labs(x = 'MDS1', y = 'MDS2', 
         subtitle = paste("Metric multidimensional scaling of",
                          nrow(dogs),"dog genotypes"))
}
```

Change the file names here if necessary.

```{r mds_data}
# read and tidy up the mds coordinates
mds <- 
  read_delim("cr237_filter6.mds", delim = ' ') %>% 
  janitor::clean_names() %>% 
  mutate(across(c1:c10,  as.numeric)) %>% 
  select(id = iid, c1:c10)

# tidy up phenotype data, then join with mds components
dogs <- 
  read_delim("cr237_filter6.fam", delim = ' ', 
             col_names = paste0('X', 1:6)) %>% 
  # recode variables
  transmute(id = X1,
            sex = if_else(X5 == 1, 'M', 'F'),
            phenotype = case_when(
              X6 == 1 ~ 'Control', 
              X6 == 2 ~ 'ACL rupture', 
              TRUE ~ 'Missing')
  ) %>% 
  mutate_if(is_character, as_factor) %>% 
  # combine .fam and .mds data by id
  right_join(mds, by = 'id')

```

In the first two MDS components we can make out two groups with one larger the the other, but no individuals are extremely far from the cloud. The grouping suggests that maybe there is some population structure in the dataset that we aren't aware of. Cases and controls are mixed together, there is no pattern.

```{r make_mds1}
gwas_mds1(dogs)
```


----


#### Manhattan plot \

This creates the classic GWAS figure: the Manhattan plot from the output of plink --assoc (\*.assoc) or --logistic (\*.assoc.logistic). The p-values for each SNP are plotted on the y-axis, with SNP position along the x-axis. Some work goes into getting the x-axis to reflect chromosome lengths and to make the colors alternate. This code was adapted from [a blog post by Daniel Roelfs](https://danielroelfs.com/blog/how-i-create-manhattan-plots-using-ggplot/).

```{r Manhattan1}
gwas_manhattan_plot <- function(assoc_data, 
                                label_if, 
                                snpsOfInterest){

  assoc_data <- assoc_data %>%
    group_by(chr) %>%
    summarise(chr_len = max(bp)) %>%
    # cumulative position of each chromosome
    mutate(tot = cumsum(chr_len) - chr_len) %>%
    select(-chr_len) %>%
    # join cumulative position to initial data
    left_join(logistic, by = 'chr') %>%
    # get the cumulative position of each SNP
    arrange(chr, bp) %>%
    mutate(BPcum = bp + tot) %>%
    # Add highlight and annotation information
    mutate(is_highlight = ifelse(snp %in% snpsOfInterest, "yes", "no")) %>%
    mutate(is_annotate = ifelse(-log10(p) > label_if, "yes", "no"))

  # Prepare X axis breaks + labels
  axisdf <- assoc_data %>%
    group_by(chr) %>%
    summarize(center=(max(BPcum) + min(BPcum)) / 2)

  # Make the manhattan plot showing -logP ~ position:
  ggplot(assoc_data, aes(BPcum, -log10(p))) +
    geom_point(aes(color = as.factor(chr)), alpha = 0.8, size = 1) +
    scale_color_manual(values = rep(c("grey35", "skyblue"), 100)) +
    # Add highlighted points
    geom_point(
      data = assoc_data %>% filter(is_highlight == "yes"),
      color = "orange",
      size = 2) +
    # Add labels using ggrepel to avoid overlap
    ggrepel::geom_text_repel(
      data = assoc_data %>% filter(is_annotate == "yes"),
      aes(label = snp),
      size = 2) +
    # Set x-axis breaks + labels
    scale_x_continuous(
      label = axisdf$chr,
      breaks = axisdf$center,
      expand = c(0.01, 0.01)) +
    # Custom the theme:
    labs(x = 'position') +
    theme_bw() +
    theme(legend.position = "none",
          axis.text.x = element_text(size = 7),
          panel.border = element_blank(),
          panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank()
    )
}
```

Parse the logistic regression results (you can do the same with other .assoc results files that share the same format / header).

```{r make_manhattan}
# parse logistic regression results
logistic <- 
  read_delim("result2.assoc.logistic", delim = ' ') %>% 
  janitor::clean_names() %>% 
  mutate_all(trimws) %>% 
  transmute(chr, bp, p, snp, or, stat) %>% 
  mutate(across(c(chr, bp, p, or, stat), as.numeric))
```

Make the Manhattan plot of association p-values

```{r, fig.height=3.5, fig.width=8}
# create a vector of snps to we want to highlight in the manhattan plot (eg. based on p-value)
snpsOfInterest <- logistic %>% 
  filter(p < as.numeric('1e-5')) %>%
  pull(snp)
         
# Make the Manhattan plot using the function we created above
# the 'label_if' parameter sets the threshold -logP above which snps will get text labels
gwas_manhattan_plot(logistic, label_if = 4, snpsOfInterest) +
  # to adjust appearance of y-axis (ignore the replacement warning)
  scale_y_continuous(expand = c(0.05, 0.05), limits = c(0, 6),
                     breaks = seq(0, 6, 1)) +
  labs(subtitle = "Logistic regression test for SNP association")
```


We can also get a table of the most significant associations like so:

```{r}
# show the most significant hits
logistic %>% 
  filter(p < 0.0001) %>% 
  arrange(p) %>% 
  select(chromosome = chr,
         postion = bp,
         SNP = snp, 
         OR = or, 
         `test statistic` = stat, 
         `p-value` = p) %>% 
  kableExtra::kable(format = 'simple',
                    caption = 'Most significant results')
```

#### Quantile-quantile plot \

This section creates the QQ plot for the logistic regression p-values in the Manhattan above. This is showing us the quantiles of the distribution of observed p-values against the quantiles of the distribution expected by chance alone. Since the fit is very close to the expected, this indicates that we don't have any association signal in our data. This is not surprising given that this dataset is very small, but it is a bit sad since we did try our best. If there were strong associations in the data, we would see the most significant observed p-values shoot upwards from the diagonal line. If shape along the diagonal line has unusual curvature in the low-significance quartiles, then there is something very wrong with the analysis (eg. population stratification).

This code adapted from [a blog post by Daniel Roelfs](https://danielroelfs.com/blog/how-i-make-qq-plots-using-ggplot/).

```{r QQplot}
gwas_qq_plot <- function(assoc_data){

  # set a confidence interval range
  ci <- 0.95
  # count number of SNPs tested
  nSNPs <- nrow(assoc_data)

  # create a sorted vector of p-values,
  # use the ppoints function to get the expected distribution
  plotdata <- 
    tibble(
      observed = sort(assoc_data$p),
      expected = ppoints(nSNPs),
      ci_lower = qbeta(p = (1 - ci) / 2,
                       shape1 = seq(nSNPs),
                       shape2 = rev(seq(nSNPs))),
      ci_upper = qbeta(p = (1 + ci) / 2,
                       shape1 = seq(nSNPs),
                       shape2 = rev(seq(nSNPs)))
      ) %>%
    mutate_all(~ -log10(.x))

  # make scatterplot
  plotdata %>%
    ggplot(aes(expected, observed)) +
    geom_abline(intercept = 0, slope = 1, lty = 2, color = 'gray') +
    geom_point(shape = 1, ) +
    coord_equal() +
    theme_classic() +
    labs(
      x = expression(Expected ~ -log[10](p)),
      y = expression(Observed ~ -log[10](p))
    )
}
```


```{r show_qq, fig.height=3.5}
gwas_qq_plot(logistic) +
  labs(title =  "Q-Q plot for logistic regression")
```


---
